{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "306a235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import get_file\n",
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "from src.factory import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b160426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base64_to_PIL_image(base64_img):\n",
    "    return Image.open(BytesIO(base64.b64decode(base64_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "152295c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL_image_to_base64(pil_image):\n",
    "    buffered = BytesIO()\n",
    "    img1=pil_image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79ca862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./sample_images/images2.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af9aa2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"https://github.com/yu4u/age-gender-estimation/releases/download/v0.6/EfficientNetB3_224_weights.11-3.44.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83526c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/yu4u/age-gender-estimation/releases/download/v0.6/EfficientNetB3_224_weights.11-3.44.hdf5\n",
      "132434532/132434532 [==============================] - 126s 1us/step\n"
     ]
    }
   ],
   "source": [
    "modhash = '6d7f7b7ced093a8b3ef6399163da6ece'\n",
    "weight_file = get_file(\"EfficientNetB3_224_weights.11-3.44.hdf5\", pretrained_model,\n",
    "                       cache_subdir=\"pretrained_models\",\n",
    "                       file_hash=modhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89a72a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, img_size = Path(weight_file).stem.split(\"_\")[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e05b973e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'224'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6e8459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = base64_to_PIL_image(image_string)\n",
    "img = np.array(image)\n",
    "input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "input_img = cv2.flip(input_img, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "975bf41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463, 662, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d24e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "78ac792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h, img_w, _ = np.shape(input_img)\n",
    "# detect faces using dlib detector\n",
    "detected = detector(input_img, 1)\n",
    "img_size=int(img_size)\n",
    "faces = np.empty((len(detected),img_size,img_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95b971af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rectangles[[(253, 98) (408, 253)]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1a0a221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8858b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"This script detects faces from web cam input, \"\n",
    "                                                 \"and estimates age and gender for the detected faces.\",\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--weight_file\", type=str, default=None,\n",
    "                        help=\"path to weight file (e.g. weights.28-3.73.hdf5)\")\n",
    "    parser.add_argument(\"--margin\", type=float, default=0.4,\n",
    "                        help=\"margin around detected face for age-gender estimation\")\n",
    "    parser.add_argument(\"--image_dir\", type=str, default=None,\n",
    "                        help=\"target image directory; if set, images in image_dir are used instead of webcam\")\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8589bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = get_args()\n",
    "# weight_file = args.weight_file\n",
    "margin = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b87aa09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xw1,yw1 (190, 35), xw2,yw2 (471, 316)\n",
      "(463, 662, 3)\n"
     ]
    }
   ],
   "source": [
    "for i, d in enumerate(detected):\n",
    "    x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "    xw1 = max(int(x1 - margin * w), 0)\n",
    "    yw1 = max(int(y1 - margin * h), 0)\n",
    "    xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "    yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "    print(f\"xw1,yw1 {xw1,yw1}, xw2,yw2 {xw2,yw2}\")\n",
    "    cv2.rectangle(input_img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "    # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
    "    print(input_img.shape)\n",
    "    faces[i] = cv2.resize(input_img, (img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "707fc9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.from_dotlist([\"model.model_name=EfficientNetB3\", \"model.img_size=224\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4455b4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 12:29:31.224224: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/raghavs/anaconda3/envs/cv_env/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2024-05-29 12:29:31.224258: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-05-29 12:29:31.224280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (smartData-442): /proc/driver/nvidia/version does not exist\n",
      "2024-05-29 12:29:31.224930: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3eb37c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32d7fb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "972c58e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_genders = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e3de0a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69807833, 0.30192158]], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "be15277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.arange(0, 101).reshape(101, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e086cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ages = results[1].dot(ages).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "675de1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49.51550296]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1].dot(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481d6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
